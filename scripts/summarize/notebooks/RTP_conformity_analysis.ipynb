{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72c8601",
   "metadata": {},
   "source": [
    "# Conformity Analysis\n",
    "Performed for PM2.5 Maintenance area of Pierce County. The plot below shows the geographic extent of the area. This is used to summarize running emissions. Emissions totals are calculated using the same process as regional emissions. \n",
    "\n",
    "Pierce County start emissions are scaled based on the VMT in the maintenance area versus VMT for Pierce County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f483a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from shapely import wkt\n",
    "sys.path.append(os.path.join(os.getcwd(),r'..\\..\\..'))\n",
    "import toml\n",
    "config = toml.load('../../../configuration/input_configuration.toml')\n",
    "sys.path.append(os.path.join(os.getcwd(),r'..\\..\\..\\scripts\\summarize\\standard'))\n",
    "# from standard_summary_configuration import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import toml\n",
    "config = toml.load(os.path.join(os.getcwd(),r'../../../configuration/input_configuration.toml'))\n",
    "sum_config = toml.load(os.path.join(os.getcwd(),r'../../../configuration/summary_configuration.toml'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5149b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of functions below are used for a variety of calculations. \n",
    "# These are adapted from the main emissions.py script used calculate regional emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f243940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_sde(connection_string, feature_class_name, version,\n",
    "                  crs={'init': 'epsg:2285'}, is_table = False):\n",
    "    \"\"\"\n",
    "    Returns the specified feature class as a geodataframe from ElmerGeo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    connection_string : SQL connection string that is read by geopandas \n",
    "                        read_sql function\n",
    "    \n",
    "    feature_class_name: the name of the featureclass in PSRC's ElmerGeo \n",
    "                        Geodatabase\n",
    "    \n",
    "    cs: cordinate system\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    engine = sqlalchemy.create_engine(connection_string)\n",
    "    con=engine.connect()\n",
    "    #con.execute(\"sde.set_current_version {0}\".format(version))\n",
    "    if is_table:\n",
    "        gdf=pd.read_sql('select * from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "    else:\n",
    "        df=pd.read_sql('select *, Shape.STAsText() as geometry from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "        df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "        gdf=gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = crs\n",
    "        cols = [col for col in gdf.columns if col not in \n",
    "                ['Shape', 'GDB_GEOMATTR_DATA', 'SDE_STATE_ID']]\n",
    "        gdf = gdf[cols]\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "def grams_to_tons(value):\n",
    "    \"\"\" Convert grams to tons.\"\"\"\n",
    "\n",
    "    value = value/453.592\n",
    "    value = value/2000\n",
    "\n",
    "    return value\n",
    "\n",
    "def calculate_interzonal_vmt(df):\n",
    "    \"\"\" Calcualte inter-zonal running emission rates from network outputs. \"\"\"\n",
    "\n",
    "    # List of vehicle types to include in results; note that bus is included here but not for intrazonals\n",
    "    vehicle_type_list = ['sov','hov2','hov3','bus','medium_truck','heavy_truck']\n",
    "\n",
    "    # Apply county names\n",
    "    county_id_lookup = {\n",
    "        33: 'king',\n",
    "        35: 'kitsap',\n",
    "        53: 'pierce',\n",
    "        61: 'snohomish'\n",
    "    }\n",
    "\n",
    "    df['geog_name'] = df['@countyid'].map(county_id_lookup)\n",
    "\n",
    "    # Remove links with facility type = 0 from the calculation\n",
    "    df['facility_type'] = df['data3']    # Rename for human readability\n",
    "    df = df[df['facility_type'] > 0]\n",
    "\n",
    "    # Calculate VMT by bus, SOV, HOV2, HOV3+, medium truck, heavy truck\n",
    "    df['sov_vol'] = df['@sov_inc1']+df['@sov_inc2']+df['@sov_inc3']\n",
    "    df['sov_vmt'] = df['sov_vol']*df['length']\n",
    "    df['hov2_vol'] = df['@hov2_inc1']+df['@hov2_inc2']+df['@hov2_inc3']\n",
    "    df['hov2_vmt'] = df['hov2_vol']*df['length']\n",
    "    df['hov3_vol'] = df['@hov3_inc1']+df['@hov3_inc2']+df['@hov3_inc3']\n",
    "    df['hov3_vmt'] = df['hov3_vol']*df['length']\n",
    "    df['bus_vmt'] = df['@bveh']*df['length']\n",
    "    df['medium_truck_vmt'] = df['@mveh']*df['length']\n",
    "    df['heavy_truck_vmt'] = df['@hveh']*df['length']\n",
    "\n",
    "    # Convert TOD periods into hours used in emission rate files\n",
    "    df['hourId'] = df['tod'].map(sum_config['tod_lookup']).astype('int')\n",
    "\n",
    "    # Calculate congested speed to separate time-of-day link results into speed bins\n",
    "    df['congested_speed'] = (df['length']/df['auto_time'])*60\n",
    "    df['avgspeedbinId'] = pd.cut(df['congested_speed'], sum_config['speed_bins'], labels=range(1, len(sum_config['speed_bins']))).astype('int')\n",
    "\n",
    "    # Relate soundcast facility types to emission rate definitions (e.g., minor arterial, freeway)\n",
    "    df['roadtypeId'] = df[\"facility_type\"].map(sum_config['fac_type_lookup']).astype('int')\n",
    "\n",
    "    # Take total across columns where distinct emission rate are available\n",
    "    # This calculates total VMT, by vehicle type (e.g., HOV3 VMT for hour 8, freeway, King County, 55-59 mph)\n",
    "    join_cols = ['avgspeedbinId','roadtypeId','hourId','geog_name']\n",
    "    df = df.groupby(join_cols).sum()\n",
    "    df = df[['sov_vmt','hov2_vmt','hov3_vmt','bus_vmt','medium_truck_vmt','heavy_truck_vmt']]\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Write this file for calculation with different emission rates\n",
    "    df.to_csv(r'../../../outputs/emissions/conformity/interzonal_vmt_grouped.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def finalize_emissions(df, col_suffix=\"\"):\n",
    "    \"\"\" \n",
    "    Compute PM10 and PM2.5 totals, sort index by pollutant value, and pollutant name.\n",
    "    For total columns add col_suffix (e.g., col_suffix='intrazonal_tons')\n",
    "    \"\"\"\n",
    "\n",
    "    pm10 = df[df['pollutantID'].isin([100,106,107])].groupby('veh_type').sum().reset_index()\n",
    "    pm10['pollutantID'] = 'PM10'\n",
    "    pm25 = df[df['pollutantID'].isin([110,116,117])].groupby('veh_type').sum().reset_index()\n",
    "    pm25['pollutantID'] = 'PM25'\n",
    "    df = df.append(pm10)\n",
    "    df = df.append(pm25)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_interzonal_emissions(df, df_rates):\n",
    "    \"\"\" Calculate link emissions using rates unique to speed, road type, hour, county, and vehicle type. \"\"\"\n",
    "\n",
    "    df.rename(columns={'geog_name':'county', 'avgspeedbinId': 'avgSpeedBinID', 'roadtypeId': 'roadTypeID', 'hourId': 'hourID'}, inplace=True)\n",
    "\n",
    "    # Calculate total VMT by vehicle group\n",
    "    df['light'] = df['sov_vmt']+df['hov2_vmt']+df['hov3_vmt']\n",
    "    df['medium'] = df['medium_truck_vmt']\n",
    "    df['heavy'] = df['heavy_truck_vmt']\n",
    "    # What about buses??\n",
    "    df.drop(['sov_vmt','hov2_vmt','hov3_vmt','medium_truck_vmt','heavy_truck_vmt','bus_vmt'], inplace=True, axis=1)\n",
    "\n",
    "    # Melt to pivot vmt by vehicle type columns as rows\n",
    "    df = pd.melt(df, id_vars=['avgSpeedBinID','roadTypeID','hourID','county'], var_name='veh_type', value_name='vmt')\n",
    "\n",
    "    df = pd.merge(df, df_rates, on=['avgSpeedBinID','roadTypeID','hourID','county','veh_type'], how='left', left_index=False)\n",
    "    # Calculate total grams of emission \n",
    "    df['grams_tot'] = df['grams_per_mile']*df['vmt']\n",
    "    df['tons_tot'] = grams_to_tons(df['grams_tot'])\n",
    "\n",
    "    df.to_csv(r'..\\..\\..\\outputs\\emissions\\conformity\\interzonal_emissions.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_intrazonal_vmt():\n",
    "    \"\"\" Calculate VMT on trips within the same zones, for a specified maintentance area.\"\"\"\n",
    "\n",
    "    df_iz = pd.read_csv(r'../../../outputs/network/iz_vol.csv')\n",
    "    \n",
    "    # Select relevant zones from the maintenance area network\n",
    "    df_iz = df_iz[df_iz['taz'].isin(taz_maint['taz'])]\n",
    "\n",
    "    # Map each zone to county\n",
    "    county_df = pd.read_sql('SELECT * FROM taz_geography', con=conn)\n",
    "    df_iz = pd.merge(df_iz, county_df, how='left', on='taz')\n",
    "\n",
    "    # Sum up SOV, HOV2, and HOV3 volumes across user classes 1, 2, and 3 by time of day\n",
    "    # Calcualte VMT for these trips too; rename truck volumes for clarity\n",
    "    for tod in sum_config['tod_lookup'].keys():\n",
    "        df_iz['sov_'+tod+'_vol'] = df_iz['sov_inc1_'+tod]+df_iz['sov_inc2_'+tod]+df_iz['sov_inc3_'+tod]\n",
    "        df_iz['hov2_'+tod+'_vol'] = df_iz['hov2_inc1_'+tod]+df_iz['hov2_inc2_'+tod]+df_iz['hov2_inc3_'+tod]\n",
    "        df_iz['hov3_'+tod+'_vol'] = df_iz['hov3_inc1_'+tod]+df_iz['hov3_inc2_'+tod]+df_iz['hov3_inc3_'+tod]\n",
    "        df_iz['mediumtruck_'+tod+'_vol'] = df_iz['medium_truck_'+tod]\n",
    "        df_iz['heavytruck_'+tod+'_vol'] = df_iz['heavy_truck_'+tod]\n",
    "\n",
    "        # Calculate VMT as intrazonal distance times volumes \n",
    "        df_iz['sov_'+tod+'_vmt'] = df_iz['sov_'+tod+'_vol']*df_iz['izdist']\n",
    "        df_iz['hov2_'+tod+'_vmt'] = df_iz['hov2_'+tod+'_vol']*df_iz['izdist']\n",
    "        df_iz['hov3_'+tod+'_vmt'] = df_iz['hov3_'+tod+'_vol']*df_iz['izdist']\n",
    "        df_iz['mediumtruck_'+tod+'_vmt'] = df_iz['mediumtruck_'+tod+'_vol']*df_iz['izdist']\n",
    "        df_iz['heavytruck_'+tod+'_vmt'] = df_iz['heavytruck_'+tod+'_vol']*df_iz['izdist']\n",
    "\n",
    "    # Group totals by vehicle type, time-of-day, and county\n",
    "    df = df_iz.groupby('geog_name').sum().T\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[df['index'].apply(lambda row: 'vmt' in row)]\n",
    "#     df.columns = ['index','King','Kitsap','Pierce','Snohomish']\n",
    "    df.columns = ['index']+[i.split(' County')[0] for i in df_iz['geog_name'].unique()]\n",
    "\n",
    "    # Calculate total VMT by time of day and vehicle type\n",
    "    # Ugly dataframe reformatting to unstack data\n",
    "    df['tod'] = df['index'].apply(lambda row: row.split('_')[1])\n",
    "    df['vehicle_type'] = df['index'].apply(lambda row: row.split('_')[0])\n",
    "    df.drop('index', axis=1,inplace=True)\n",
    "    df.index = df[['tod','vehicle_type']]\n",
    "    df.drop(['tod','vehicle_type'],axis=1,inplace=True)\n",
    "    df = pd.DataFrame(df.unstack()).reset_index()\n",
    "    df['tod'] = df['level_1'].apply(lambda row: row[0])\n",
    "    df['vehicle_type'] = df['level_1'].apply(lambda row: row[1])\n",
    "    df.drop('level_1', axis=1, inplace=True)\n",
    "    df.columns = ['geog_name','VMT','tod','vehicle_type']\n",
    "\n",
    "    # Use hourly periods from emission rate files\n",
    "    df['hourId'] = df['tod'].map(sum_config['tod_lookup']).astype('int')\n",
    "\n",
    "    # Export this file for use with other rate calculations\n",
    "    # Includes total VMT for each group for which rates are available\n",
    "    df.to_csv(r'../../../outputs/emissions/conformity/intrazonal_vmt_grouped.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_intrazonal_emissions(df_running_rates):\n",
    "    \"\"\" Summarize intrazonal emissions by vehicle type. \"\"\"\n",
    "\n",
    "    df_intra = pd.read_csv(r'../../../outputs/emissions/conformity/intrazonal_vmt_grouped.csv')\n",
    "    df_intra.rename(columns={'vehicle_type':'veh_type', 'VMT': 'vmt', 'hourId': 'hourID', 'geog_name': 'county'},inplace=True)\n",
    "    df_intra.drop('tod', axis=1, inplace=True)\n",
    "    df_intra['county'] = df_intra['county'].apply(lambda row: row.lower())\n",
    "\n",
    "    df_intra_light = df_intra[df_intra['veh_type'].isin(['sov','hov2','hov3'])]\n",
    "    df_intra_light = df_intra_light.groupby(['county','hourID']).sum()[['vmt']].reset_index()\n",
    "    df_intra_light.loc[:,'veh_type'] = 'light'\n",
    "\n",
    "    df_intra_medium = df_intra[df_intra['veh_type'] == 'mediumtruck']\n",
    "    df_intra_medium.loc[:,'veh_type'] = 'medium'\n",
    "    df_intra_heavy = df_intra[df_intra['veh_type'] == 'heavytruck']\n",
    "    df_intra_heavy.loc[:,'veh_type'] = 'heavy'\n",
    "\n",
    "    df_intra = df_intra_light.append(df_intra_medium)\n",
    "    df_intra = df_intra.append(df_intra_heavy)\n",
    "\n",
    "    # For intrazonals, assume standard speed bin and roadway type for all intrazonal trips\n",
    "    speedbin = 4\n",
    "    roadtype = 5\n",
    "\n",
    "    iz_rates = df_running_rates[(df_running_rates['avgSpeedBinID'] == speedbin) &\n",
    "                        (df_running_rates['roadTypeID'] == roadtype)]\n",
    "\n",
    "    df_intra = pd.merge(df_intra, iz_rates, on=['hourID','county','veh_type'], how='left', left_index=False)\n",
    "\n",
    "    # Calculate total grams of emission \n",
    "    df_intra['grams_tot'] = df_intra['grams_per_mile']*df_intra['vmt']\n",
    "    df_intra['tons_tot'] = grams_to_tons(df_intra['grams_tot'])\n",
    "\n",
    "    # Write raw output to file\n",
    "    df_intra.to_csv(r'../../../outputs/emissions/conformity/intrazonal_emissions.csv', index=False)\n",
    "\n",
    "    return df_intra\n",
    "\n",
    "def calculate_start_emissions():\n",
    "    \"\"\" Calculate start emissions based on vehicle population by county and year. \"\"\"\n",
    "\n",
    "    df_veh = pd.read_sql('SELECT * FROM vehicle_population WHERE year=='+config['base_year'], con=conn)\n",
    "\n",
    "    # Scale all vehicles by difference between base year and model total vehicles owned from auto onwership model\n",
    "    df_hh = pd.read_csv(r'../../../outputs/daysim/_household.tsv', delim_whitespace=True, usecols=['hhvehs'])\n",
    "    tot_veh = df_hh['hhvehs'].sum()\n",
    "\n",
    "    # Scale county vehicles by total change\n",
    "    tot_veh_model_base_year = 3007056\n",
    "    veh_scale = 1.0+(tot_veh - tot_veh_model_base_year)/tot_veh_model_base_year\n",
    "    df_veh['vehicles'] = df_veh['vehicles']*veh_scale\n",
    "\n",
    "    # Join with rates to calculate total emissions\n",
    "    start_rates_df = pd.read_sql('SELECT * FROM start_emission_rates_by_veh_type WHERE year=='+config['model_year'], con=conn)\n",
    "    \n",
    "    # Select winter rates for pollutants other than those listed in summer_list\n",
    "    df_summer = start_rates_df[start_rates_df['pollutantID'].isin(sum_config['summer_list'])]\n",
    "    df_summer = df_summer[df_summer['monthID'] == 7]\n",
    "    df_winter = start_rates_df[~start_rates_df['pollutantID'].isin(sum_config['summer_list'])]\n",
    "    df_winter = df_winter[df_winter['monthID'] == 1]\n",
    "    start_rates_df = df_winter.append(df_summer)\n",
    "\n",
    "    # Sum total emissions across all times of day, by county, for each pollutant\n",
    "    start_rates_df = start_rates_df.groupby(['pollutantID','county','veh_type']).sum()[['ratePerVehicle']].reset_index()\n",
    "    \n",
    "    df = pd.merge(df_veh, start_rates_df, left_on=['type','county'],right_on=['veh_type','county'])\n",
    "    df['start_grams'] = df['vehicles']*df['ratePerVehicle'] \n",
    "    df['start_tons'] = grams_to_tons(df['start_grams'])\n",
    "    df = df.groupby(['pollutantID','veh_type','county']).sum().reset_index()\n",
    "\n",
    "    df.to_csv(r'../../../outputs/emissions/conformity/start_emissions.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb996718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  TAZ geographies from ElmerGeo\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "version = \"'DBO.Default'\"\n",
    "taz_gdf = read_from_sde(connection_string, 'taz2010', version, crs=crs, is_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36c25dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "X:\\Trans\\TIP\\AIRQUAL\\Conformity\\pm25_harn.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m  Traceback (most recent call last)",
      "\u001b[1;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: X:\\Trans\\TIP\\AIRQUAL\\Conformity\\pm25_harn.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDriverError\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0d2c1ad5566a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the PM2.5 conformity shapefile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgdf_pm25\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'X:\\Trans\\TIP\\AIRQUAL\\Conformity\\pm25_harn.shp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgdf_pm25\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgdf_pm25\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2285\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\summary\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\summary\\lib\\site-packages\\fiona\\env.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\summary\\lib\\site-packages\\fiona\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[1;32m--> 254\u001b[1;33m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\summary\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: X:\\Trans\\TIP\\AIRQUAL\\Conformity\\pm25_harn.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Load the PM2.5 conformity shapefile\n",
    "gdf_pm25 = gpd.read_file(r'X:\\Trans\\TIP\\AIRQUAL\\Conformity\\pm25_harn.shp')\n",
    "gdf_pm25 = gdf_pm25.to_crs(2285)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea3849",
   "metadata": {},
   "source": [
    "PM2.5 Maintenance Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ade82",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pm25.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network shapefile to intersect network with conformity areas\n",
    "gdf_network = gpd.read_file(r'../../../inputs/scenario/networks/shapefiles/AM/AM_edges.shp')\n",
    "\n",
    "# Intersect network links with conformity geography\n",
    "gdf_intersect = gpd.overlay(gdf_network, gdf_pm25, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e28a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intesect conformity area with TAZ file to get list of TAZs for intrazonal calculations\n",
    "taz_maint = gpd.overlay(gdf_pm25, taz_gdf, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taz_maint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1ce48",
   "metadata": {},
   "source": [
    "TAZs in Maintenance Area. The intrazonal emissions totals will be included for these TAZs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f614f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_maint.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a2b95",
   "metadata": {},
   "source": [
    "Network within PM2.5 Maintenance Area.\n",
    "\n",
    "These network links are used to calculate running (interzonal) emissions totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af97874",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_intersect.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network results with data on VMT, speed, facility type, etc. \n",
    "network_df = pd.read_csv(r'../../../outputs/network/network_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9118d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join network data to the PM2.5 maintenance area network\n",
    "network_pm25_df = gdf_intersect[['id']].merge(network_df, left_on='id', right_on='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7531141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output subdirectory for conformity emissions results \n",
    "if not os.path.isdir('../../../outputs/emissions/conformity'):\n",
    "    os.makedirs('../../../outputs/emissions/conformity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ea648",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_engine('sqlite:///../../../inputs/db/'+config['db_name'])\n",
    "\n",
    "# Load running emission rates by vehicle type, for the model year\n",
    "df_running_rates = pd.read_sql('SELECT * FROM running_emission_rates_by_veh_type WHERE year=='+config['model_year'], con=conn)\n",
    "df_running_rates.rename(columns={'ratePerDistance': 'grams_per_mile'}, inplace=True)\n",
    "df_running_rates['year'] = df_running_rates['year'].astype('str')\n",
    "\n",
    "# Select the month to use for each pollutant; some rates are used for winter or summer depending\n",
    "# on when the impacts are at a maximum due to temperature.\n",
    "df_summer = df_running_rates[df_running_rates['pollutantID'].isin(sum_config['summer_list'])]\n",
    "df_summer = df_summer[df_summer['monthID'] == 7]\n",
    "df_winter = df_running_rates[~df_running_rates['pollutantID'].isin(sum_config['summer_list'])]\n",
    "df_winter = df_winter[df_winter['monthID'] == 1]\n",
    "df_running_rates = df_winter.append(df_summer)\n",
    "\n",
    "# Group interzonal trips and calculate interzonal emissions\n",
    "df_interzonal_vmt = calculate_interzonal_vmt(network_pm25_df)\n",
    "df_interzonal = calculate_interzonal_emissions(df_interzonal_vmt, df_running_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c80340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate start emissions by vehicle type\n",
    "start_emissions_df = calculate_start_emissions()\n",
    "\n",
    "# Select only emissions for Pierce County, the location of the maintenance area\n",
    "start_emissions_df = start_emissions_df[start_emissions_df['county'] == 'pierce']\n",
    "\n",
    "# Start emissions are based on vehicles per county\n",
    "# Since the PM Maintenance area is only a portion of Pierce County\n",
    "# we need to apply a scaling factor to represent only the study area\n",
    "# In previous analyses we used % VMT in the study area compared to Pierce County overall\n",
    "\n",
    "# calculate total VMT in the maintenance area\n",
    "network_pm25_df['VMT'] = network_pm25_df['@tveh']*network_pm25_df['length']\n",
    "maint_vmt = network_pm25_df['VMT'].sum()\n",
    "\n",
    "# Apply county names\n",
    "county_id_lookup = {\n",
    "    33: 'king',\n",
    "    35: 'kitsap',\n",
    "    53: 'pierce',\n",
    "    61: 'snohomish'\n",
    "}\n",
    "\n",
    "# Calculate total VMT for pierce county\n",
    "network_df['VMT'] = network_df['@tveh']*network_df['length']\n",
    "network_df['county_name'] = network_df['@countyid'].map(county_id_lookup)\n",
    "pierce_vmt = network_df[network_df['county_name'] == 'pierce']['VMT'].sum()\n",
    "\n",
    "starts_scaling_factor = maint_vmt/pierce_vmt\n",
    "\n",
    "# Apply scaling factor to the starts emissions totals\n",
    "start_emissions_df['start_tons'] = start_emissions_df['start_tons']*starts_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group intrazonal trips and calculate intrazonal emissions\n",
    "df_intrazonal_vmt = calculate_intrazonal_vmt()\n",
    "df_intrazonal = calculate_intrazonal_emissions(df_running_rates)\n",
    "\n",
    "# Combine all rates and export as CSV\n",
    "df_inter_group = df_interzonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot']].reset_index()\n",
    "df_inter_group.rename(columns={'tons_tot': 'interzonal_tons'}, inplace=True)\n",
    "df_intra_group = df_intrazonal.groupby(['pollutantID','veh_type']).sum()[['tons_tot']].reset_index()\n",
    "df_intra_group.rename(columns={'tons_tot': 'intrazonal_tons'}, inplace=True)\n",
    "df_start_group = start_emissions_df.groupby(['pollutantID','veh_type']).sum()[['start_tons']].reset_index()\n",
    "\n",
    "summary_df = pd.merge(df_inter_group, df_intra_group)\n",
    "summary_df = pd.merge(summary_df, df_start_group, how='left')\n",
    "summary_df = finalize_emissions(summary_df, col_suffix=\"\")\n",
    "summary_df.loc[~summary_df['pollutantID'].isin(['PM','PM10','PM25']),'pollutantID'] = summary_df[~summary_df['pollutantID'].isin(['PM','PM10','PM25'])]['pollutantID'].astype('int')\n",
    "summary_df['pollutant_name'] = summary_df['pollutantID'].astype('int', errors='ignore').astype('str').map(sum_config['pollutant_map'])\n",
    "summary_df['total_daily_tons'] = summary_df['start_tons']+summary_df['interzonal_tons']+summary_df['intrazonal_tons']\n",
    "summary_df = summary_df[['pollutantID','pollutant_name','veh_type','start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]\n",
    "summary_df.to_csv(r'../../../outputs/emissions/conformity/emissions_summary.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddffbab2",
   "metadata": {},
   "source": [
    "# PM2.5\n",
    "Tons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce16db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = summary_df[summary_df['pollutant_name'] == 'PM25 Total']\n",
    "df = df[['veh_type','start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]\n",
    "df.index = df['veh_type']\n",
    "df[['start_tons','intrazonal_tons','total_daily_tons']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144f0bd",
   "metadata": {},
   "source": [
    "Pounds in Maintenance Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f904e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs = df[['start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]*2000\n",
    "df_lbs.rename(columns={'start_tons': 'start_pounds', 'intrazonal_tons': 'intrazonal_pounds', \n",
    "                       'total_daily_tons': 'total_daily_pounds', 'interzonal_tons': 'interzonal_pounds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b70f6",
   "metadata": {},
   "source": [
    "Total Pounds in Mainentance Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs['total_daily_pounds'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d53fd0",
   "metadata": {},
   "source": [
    "# NOx\n",
    "Tons in Maintenance Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = summary_df[summary_df['pollutant_name'] == 'NOx']\n",
    "df = df[['veh_type','start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]\n",
    "df.index = df['veh_type']\n",
    "df[['start_tons','intrazonal_tons','total_daily_tons']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b37fbb",
   "metadata": {},
   "source": [
    "Pounds in Maintenance Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs = df[['start_tons','intrazonal_tons','interzonal_tons','total_daily_tons']]*2000\n",
    "df_lbs.rename(columns={'start_tons': 'start_pounds', 'intrazonal_tons': 'intrazonal_pounds', \n",
    "                       'total_daily_tons': 'total_daily_pounds', 'interzonal_tons': 'interzonal_pounds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cf17c",
   "metadata": {},
   "source": [
    "Total Pounds in Maitenance Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5281a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbs['total_daily_pounds'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
